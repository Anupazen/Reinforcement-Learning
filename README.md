# Reinforcement-Learning

Reository created for Reinforcement-Learning codings

<img width="384" height="243" alt="image" src="https://github.com/user-attachments/assets/fa3122eb-0a9e-49a6-8a2f-ea5ff04e08c7" />


The rules of the environment are as follows.
1. An agent is supposed to start in cell (1,1).
2. Available actions, ğ´(ğ‘ ) = {ğ‘¢ğ‘, ğ‘‘ğ‘œğ‘¤ğ‘›, ğ‘™ğ‘’ğ‘“ğ‘¡, ğ‘Ÿğ‘–ğ‘”â„ğ‘¡}, where state ğ‘  is some cell (ğ‘¥, ğ‘¦) with ğ‘¥ being 
the column number and ğ‘¦ being the row number.
a. An action results in the intended movement with probability 0.8 but can cause movement 
in one of the perpendicular directions with probability 0.1 each.
b. Bumping into a wall results in staying in the same cell.
3. ğ‘…(ğ‘ ) denotes the reward received in state ğ‘ . ğ‘…((4,3)) = +1,ğ‘…((4,2)) =
âˆ’1, ğ‘ğ‘›ğ‘‘ ğ‘“ğ‘œğ‘Ÿ ğ‘ğ‘›ğ‘¦ ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ (ğ‘¥, ğ‘¦), ğ‘…(ğ‘¥, ğ‘¦) = âˆ’0.04.
4. The agent will exit the environment if any of the two terminal states, i.e., (4,3) ğ‘œğ‘Ÿ (4,2), is 
reached.
5. The objective of the agent is to maximize the total sum of rewards received. Assume that rewards 
are additive. Use a discount factor ğ›¾ = 0.9, wherever applicable.
6. The environment is fully observable â€“ the agent knows exactly which sell it is in
